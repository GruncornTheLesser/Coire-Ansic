
//TODO: seperate element and entity for deferred operations
//		must store linked list or something to update versions
//		for archetype and uniontype the movement of the value would denote a change in bucket which should call a construct/destroy
//		entt uses an implicit linked list in the place of the destroyed entity
//		issue with entt version is it requires the sparse set to navigate
// 		I would like to avoid using the index lookup to update where possible
//		I would like to replace the entity version pair with a simple index to the next hole
//		the deferred storage doesnt know what its erasing, could cause issues for inplace sorting though

//TODO: bucket sort pool for archetype and uniontype storage policy -> 
//		  pool buckets <A, B, C, D, AB, AC, AD, BC, BD, CD, ABC, ABD, ACD, BCD, ABCD>
//		  to check whether an entity has a component you find if the index is within the range from valid buckets
//		  valid buckets for inc<A,B> = <-, -, -, -, AB, -, -, -, -, -, ABC, ABD, -, -, ABCD>
//		  
//		  when reordering pools inplace/swap policy applies to the bucket
//		  eg entity in bucket ABC drops to bucket AB = <-, -, -, -, AB, -, -, -, -, -, -, ABD, -, -, ->
//		  AB = [..., x], ABD = [..., x, ...]
//
//		  alternatively defer the bucket calculation, ie queue bucket sort
//		  flag entity with new state
// Horrid idea! lets do it!




//TODO: explore possibilities of a swap pool -> create multiple buffer for entities, this is a binary semaphore (kinda) not sure how they are normally implemented
//		  if I were to do semaphore behaviour with several clones of the entity/component array
//		  big risk of a data race/lost processing -> its not trivial to track changes
//		  this would 100% require a version per entity -> to merge the changes
//		  I would probably want some kind of default initialization
//		  this is a slightly insane level of multithreading which would likely be entirely useless
//		  it might also be simpler to do command buffers if I want this level of insanity

2(+1 per component) Mutexes!!!
one for sparse set, one for packed, one for buffer. yippeeeee, cluster fucking right here!!!
	this would allow multiples threads to update components while another emplaces/erases/moves entities.
	the component entity can be read only, the sparse set entity can be used to insert and erase entities
	pros:
		read + write concurrency -> const comp + const index, entity + const comp
		less lockup from write operations -> packed + const sparse, buffer 
		adds alot of control for how a policy should interact with the pool eg deferred add does not need buffer access at all until sync
	cons:
		more mutex locking overhead
		added complexity
		more difficult to sync -> multiple threads will try could be trying to sync at once
		less intuitive -> when a resource is written to and requires a sync its not obvious which way around that happens. could be seen as an inbuilt syntax for deferred operations but gross
		and there is still the confusion of emplacing a component and it doesnt exist until startup
	
	1(+1 per buffer) mutex
	1 mutex for sparse_set + packed
	1 mutex for per buffer


	pipeline<pool<A>::comp, pool<A>::index, pool<A>::entity, pool<A>> 
		cna also create symbols which expand to mean access locks eg
	pipeline<read<A>, write<A>, reorder<A>, readwrite<A>>

	this leads to 3^3-1=26 combinations some of those might be entirely useless

	the second part about doubling the entity arrays is also maybe pointless, it doesnt speed up sync time because you still have to 
	copy the updated entities to the other buffer


// TODO: concurrent dispatcher
//		  on raising event, increment an atomic size, if capacity reached, sync with a mutex, and reserve new capacity
//
//		  there is a lot of fiddling that can be done here, for example:
//		   - dispatch immediately after invocation (immediate)
//		   - dispatch upon sender mutex release (deferred)
//		   - dispatch on reciever mutex acquired(lazy)
//		   - dispatch when capacity reached (threshold)
//		   - dispatch periodically after time elapsed (polled) -> event loop or other sillyness
//		   - dispatch on manual invoke (manual)
//		  
//		  doing an event loop would be silly.
//		  most the multithreading optimizations you can make are application specific so it would be silly to try and write a fit all solution
//		  given that, it might be best to make threading as manual as possible. 

// NOTE: maybe its better to only allow paged_block ie no unstable pointers, it costs a tiny bit more to iterate but alot more for allocation/insertion...





